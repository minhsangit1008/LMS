"""
FastAPI version of the demo analytics API.
Reads synthetic CSVs from LMS/db/sample_data (generated by generate_fake_data.py)
and exposes the same endpoints as the Node demo:

  GET /health
  GET /analytics/course-kpis?courseid=1
  GET /analytics/grade-distribution?courseid=1&activityid=1&bins=10
  GET /analytics/active-users?from=YYYY-MM-DD&to=YYYY-MM-DD
  GET /analytics/submission-latency?courseid=1
  GET /analytics/user-engagement?courseid=1&top=10

Run (from repo root):
  pip install fastapi uvicorn pandas
  uvicorn LMS.analytics.api_fastapi:app --reload --port 4000
"""
from pathlib import Path
from typing import Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException, Query
import pandas as pd

app = FastAPI(title="LMS Analytics Demo (FastAPI)")


def resolve_data_dir() -> Path:
    cwd = Path.cwd()
    cand = cwd / "LMS" / "db" / "sample_data"
    if cand.exists():
        return cand
    cand = cwd.parent / "db" / "sample_data"
    if cand.exists():
        return cand
    raise FileNotFoundError("Cannot find LMS/db/sample_data; run generate_fake_data.py first.")


DATA_DIR = resolve_data_dir()

# Load CSV once; for production consider reload if files change.
user_dim = pd.read_csv(DATA_DIR / "user_dim.csv")
course_dim = pd.read_csv(DATA_DIR / "course_dim.csv")
enrol_fact = pd.read_csv(DATA_DIR / "enrol_fact.csv", parse_dates=["enrol_time"])
grade_fact = pd.read_csv(DATA_DIR / "grade_fact.csv", parse_dates=["graded_at"])
submission_fact = pd.read_csv(
    DATA_DIR / "submission_fact.csv", parse_dates=["submitted_at", "duedate"]
)
event_log = pd.read_csv(DATA_DIR / "event_log_staging.csv")
daily_kpi = pd.read_csv(DATA_DIR / "daily_course_kpi.csv")
daily_kpi["date"] = pd.to_datetime(daily_kpi["date"])


@app.get("/health")
def health():
    return {"status": "ok", "data_dir": str(DATA_DIR)}


@app.get("/analytics/course-kpis")
def course_kpis(courseid: int = Query(..., gt=0)):
    df = daily_kpi[daily_kpi["course_id"] == courseid]
    if df.empty:
        raise HTTPException(404, "course not found in data")
    df = df.sort_values("date")
    series = df[["date", "active_users", "submissions", "completions", "avg_grade"]].copy()
    series.loc[:, "date"] = series["date"].dt.date.astype(str)
    sum_sub = series["submissions"].sum()
    sum_act = series["active_users"].sum()
    sum_comp = series["completions"].sum()
    grades = series["avg_grade"][series["avg_grade"] > 0]
    summary = {
        "completion_rate": round(sum_comp / sum_act, 2) if sum_act else 0,
        "avg_grade": round(grades.mean(), 2) if len(grades) else 0,
        "submissions": int(sum_sub),
        "active_users": int(series["active_users"].max()),
    }
    return {"summary": summary, "series": series.to_dict(orient="records")}


@app.get("/analytics/grade-distribution")
def grade_distribution(
    courseid: int = Query(..., gt=0),
    activityid: Optional[int] = None,
    bins: int = Query(10, gt=1, le=50),
):
    df = grade_fact[grade_fact["course_id"] == courseid]
    if activityid:
        df = df[df["item_id"] == activityid]
    scores = df["score"].astype(float)
    if scores.empty:
        raise HTTPException(404, "no grade data")
    # Use numpy histogram for robustness
    import numpy as np
    counts, edges = np.histogram(scores, bins=bins, range=(0, 100))
    bins_out = []
    for i in range(len(counts)):
        bins_out.append(
            {
                "range": f"{round(edges[i],2)}-{round(edges[i+1],2)}",
                "count": int(counts[i]),
            }
        )
    return {
        "bins": bins_out,
        "mean": round(scores.mean(), 2),
        "median": round(scores.median(), 2),
    }


@app.get("/analytics/active-users")
def active_users(
    frm: Optional[str] = Query(None, alias="from"),
    to: Optional[str] = None,
):
    df = daily_kpi.copy()
    if frm:
        df = df[df["date"] >= pd.to_datetime(frm)]
    if to:
        df = df[df["date"] <= pd.to_datetime(to)]
    series = (
        df.groupby("date")["active_users"].sum().reset_index().sort_values("date")
    )
    series["date"] = series["date"].dt.date.astype(str)
    total = int(series["active_users"].sum())
    return {"series": series.to_dict(orient="records"), "total": total}


@app.get("/analytics/submission-latency")
def submission_latency(courseid: int = Query(..., gt=0)):
    df = submission_fact[submission_fact["course_id"] == courseid].copy()
    if df.empty:
        raise HTTPException(404, "no submissions")
    df["latency_hours"] = (df["submitted_at"] - df["duedate"]).dt.total_seconds() / 3600
    latencies = df["latency_hours"]
    by_activity = df.groupby("activity_id")["latency_hours"].mean().reset_index()
    items = [
        {"activity_id": int(row.activity_id), "avg_hours": round(row.latency_hours, 2)}
        for row in by_activity.itertuples()
    ]
    return {
        "avg_hours": round(latencies.mean(), 2),
        "p90_hours": round(latencies.quantile(0.9), 2),
        "items": items,
    }


@app.get("/analytics/user-engagement")
def user_engagement(courseid: Optional[int] = None, top: int = Query(10, gt=0, le=100)):
    df = event_log.copy()
    if courseid:
        df = df[df["course_id"] == courseid]
    if df.empty:
        raise HTTPException(404, "no events")
    counts = df.groupby("user_id").size().reset_index(name="events")
    top_users = (
        counts.sort_values("events", ascending=False)
        .head(top)
        .assign(user_id=lambda d: d["user_id"].astype(int))
    )
    return {"top_users": top_users.to_dict(orient="records")}
